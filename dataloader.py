# -*- coding: utf-8 -*-
"""DataLoader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x_KSjTde07kycOTjoqPGVryRwYgJWoDi
"""

import torch
import os
import json
import cv2
import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

class PoseTrackDataset(Dataset):
  def __init__(self, main_folder, json_folder, max_frames, interp_shape):
      self.main_folder = main_folder
      self.json_folder = json_folder
      self.subdirectories = sorted(next(os.walk(main_folder))[1])
      self.valid_subdirectories = [
          subdir for subdir in self.subdirectories
          if os.path.exists(os.path.join(self.json_folder, f"{subdir}.json"))
      ]
      self.max_frames = max_frames
      self.interp_shape = interp_shape

  def __len__(self):
      return len(self.valid_subdirectories)

  def make_palindrome(self, tensor, required_length):
      current_length = tensor.shape[0]
      if current_length < required_length:
          additional_frames_needed = required_length - current_length
          # Reverse the tensor along the first dimension
          mirrored_part = torch.flip(tensor, [0])
          # Repeat the mirrored part if more frames are needed
          while mirrored_part.shape[0] < additional_frames_needed:
              mirrored_part = torch.cat((mirrored_part, torch.flip(tensor, [0])), dim=0)
          mirrored_part = mirrored_part[:additional_frames_needed]
          tensor = torch.cat((tensor, mirrored_part), dim=0)
      return tensor

  def load_video(self, subdir_path, idx):
      images = sorted([img for img in os.listdir(subdir_path) if img.endswith(".jpg")])
      image_arrays = []
      for img in images:
          img_path = os.path.join(subdir_path, img)
          img_array = cv2.imread(img_path)
          image_arrays.append(img_array)

      image_arrays_np = np.array(image_arrays)
      video = torch.from_numpy(image_arrays_np).permute(0, 3, 1, 2).float()[:, [2, 1, 0], :, :]
      T, C, H, W = video.shape
      if idx + self.max_frames < T:
        video = video[idx:idx + self.max_frames]
      else:
        video = video[idx:]
        video = self.make_palindrome(video, self.max_frames)
      video = F.interpolate(video, size=self.interp_shape, mode="bilinear", align_corners=True)
      video = video.reshape(self.max_frames, 3, self.interp_shape[0], self.interp_shape[1])  # Ensure the final shape is correct

      return video, W, H


  def load_anno(self, json_path, img_path):
      def create_input_tensor(annotation):
          keypoints = annotation['keypoints']
          processed_keypoints = []
          valids = []
          frame_no = annotation['image_id'] % 1000
          for i in range(0, len(keypoints), 3):
              x = keypoints[i]
              y = keypoints[i + 1]
              processed_keypoints.append([frame_no, x, y])
              visibility = keypoints[i + 2]
              valids.append(visibility)
          return torch.tensor(processed_keypoints), torch.tensor(valids).unsqueeze(0)

      def create_keypoints_tensor(annotation):
          keypoints = annotation['keypoints']
          processed_keypoints = []
          visibility = []
          frame_no = annotation['image_id'] % 1000
          for i in range(0, len(keypoints), 3):
              x = keypoints[i]
              y = keypoints[i + 1]
              vis = keypoints[i + 2]
              processed_keypoints.append([x, y])
              visibility.append(vis)
          return torch.tensor(processed_keypoints).unsqueeze(0), torch.tensor(visibility).unsqueeze(0)

      def best_starting_frame_at_least_18(frames):
        for i in range(len(frames)):
            start_frame = frames[i]
            end_frame = start_frame + self.max_frames -1
            count = sum(1 for frame in frames if start_frame <= frame <= end_frame)

            if count >= self.max_frames/2:
                return start_frame, count

        return None, 0

      def extract_frame_number(file_name):
        base_name = os.path.basename(file_name)  # Get the base name of the file (e.g., '000142.jpg')
        frame_number = os.path.splitext(base_name)[0]  # Remove the extension (e.g., '000142')
        return int(frame_number)  # Convert to integer

      with open(json_path, 'r') as file:
          data = json.load(file)
          persons = {}
          frames = {}
          visibility = {}
          for i in data['annotations']:
              frame_num = i['image_id'] % 1000
              # print(f"Hey framenum {frame_num}")
              if i['person_id'] in persons:
                  new_annot, vis = create_keypoints_tensor(i)
                  persons[i['person_id']] = torch.cat((persons[i['person_id']], new_annot), dim=0)
                  frames[i['person_id']].append(frame_num)
                  visibility[i['person_id']] = torch.cat((visibility[i['person_id']], vis), dim=0)
                  if len(frames[i['person_id']]) == self.max_frames and frames[i['person_id']][0] + self.max_frames -1  == frames[i['person_id']][-1]:

                    initial = frames[i['person_id']][0]
                    input_frame = persons[initial]
                    frame_tensor = torch.full((17, 1), 0)
                    inputs = torch.cat((frame_tensor, input_frame), dim=1)
                    return inputs, persons[i['person_id']], visibility[i['person_id']], initial
                    # return 0, initial, initial + 35
              else:
                  # inputs[i['person_id']], valids[i['person_id']] = create_input_tensor(i)
                  persons[i['person_id']], visibility[i['person_id']] = create_keypoints_tensor(i)
                  frames[i['person_id']] = [frame_num]

          queries = None
          trajs_e = None
          start = None
          req_frames = 0
          # signal  = False
          for i in frames:
            frame_lst = frames[i]
            start, count = best_starting_frame_at_least_18(frame_lst)
            # Map frame numbers to their indices
            frame_to_index = {frame: k for k, frame in enumerate(frame_lst)}
            
            # print(f"Hey {start}")
            # print(frames[i])
            if not start is None:
              # List all files in the folder
              files = os.listdir(img_path)

              # Filter only .jpg files
              jpg_files = [f for f in files if f.endswith('.jpg')]

              # Extract frame numbers from file names and find the maximum
              frame_numbers = [extract_frame_number(os.path.join(folder_path, f)) for f in jpg_files]
              T = max(frame_numbers)
              person = i
              initial_frame = start
              # last_frame = max_frame
              num_times = T -start +1
              if num_times > 36:
                num_times = 36
              trajs_e = torch.zeros((num_times, 17, 2))
              visib = torch.zeros((num_times, 17))
              # Map frame numbers to their indices
              frame_to_index = {frame: k for k, frame in enumerate(frame_lst)}
              for k in range(num_times):
                frame_number = start + k
                if frame_number in frame_to_index:
                    trajs_e[k] = persons[person][frame_to_index[frame_number]]
                    visib[k] = visibility[i][frame_to_index[frame_number]]

              if trajs_e.shape[0] != self.max_frames:
                req_frames = self.max_frames - trajs_e.shape[0]
                trajs_e = self.make_palindrome(trajs_e, self.max_frames)
                visib = self.make_palindrome(visib, self.max_frames)

              input_frame = trajs_e[0]
              frame_tensor = torch.full((17, 1), 0)
              queries = torch.cat((frame_tensor, input_frame), dim=1)
              return queries, trajs_e, visibility, start
              # return req_frames, start, T

          if start is None:
            default_inputs = torch.zeros((17, 3))
            default_persons = torch.zeros((self.max_frames, 17, 2))
            default_visibility = torch.zeros((self.max_frames, 17))
            return default_inputs, default_persons, default_visibility, None
            # return None,0,35

  def __getitem__(self, idx):
      subdir = self.valid_subdirectories[idx]
      img_path = os.path.join(self.main_folder, subdir)
      anno_path = os.path.join(self.json_folder, f"{subdir}.json")
      # req_frames = self.load_anno(anno_path, img_path)
      queries, trajs_e, visibility, start = self.load_anno(anno_path, img_path)
      if start is None:
        video,W,H = self.load_video(img_path, 0)
      else:
        video,W,H = self.load_video(img_path, start)
      queries = queries.clone()
      queries[:, 1:] *= queries.new_tensor(
          [
              (self.interp_shape[1] - 1) / (W - 1),
              (self.interp_shape[0] - 1) / (H - 1),
          ]
      )
      queries = queries.clone()
      # Adjust tracks
      trajs_e = trajs_e.clone()
      trajs_e *= trajs_e.new_tensor(
          [
              (self.interp_shape[1] - 1) / (W - 1),
              (self.interp_shape[0] - 1) / (H - 1),
          ]
      )

      return video, queries, trajs_e, visibility

train_folder = '/content/drive/MyDrive/PoseTrack2/d1/images/train'
train_json_folder = '/content/drive/MyDrive/PoseTrack2/d1/PoseTrack21/posetrack_data/train'
train_dataset = PoseTrackDataset(train_folder, train_json_folder, 36, (384,512))

video, queries, trajs_e, visibility = train_dataset[100]

video.shape

