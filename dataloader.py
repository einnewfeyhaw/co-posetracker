# -*- coding: utf-8 -*-
"""DataLoader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x_KSjTde07kycOTjoqPGVryRwYgJWoDi
"""

import torch
import os
import json
import cv2
import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

class PoseTrackDataset(Dataset):
    def __init__(self, main_folder, json_folder, max_frames, interp_shape):
        self.main_folder = main_folder
        self.json_folder = json_folder
        self.subdirectories = sorted(next(os.walk(main_folder))[1])
        self.valid_subdirectories = [
            subdir for subdir in self.subdirectories
            if os.path.exists(os.path.join(self.json_folder, f"{subdir}.json"))
        ]
        self.max_frames = max_frames
        self.interp_shape = interp_shape

    def __len__(self):
        return len(self.valid_subdirectories)

    def make_palindrome(self, tensor, required_length):
        current_length = tensor.shape[0]
        if current_length < required_length:
            additional_frames_needed = required_length - current_length
            # Reverse the tensor along the first dimension
            mirrored_part = torch.flip(tensor, [0])
            # Repeat the mirrored part if more frames are needed
            while mirrored_part.shape[0] < additional_frames_needed:
                mirrored_part = torch.cat((mirrored_part, torch.flip(tensor, [0])), dim=0)
            mirrored_part = mirrored_part[:additional_frames_needed]
            tensor = torch.cat((tensor, mirrored_part), dim=0)
        return tensor

    def load_video(self, subdir_path, idx):
        images = sorted([img for img in os.listdir(subdir_path) if img.endswith(".jpg")])
        image_arrays = []
        for img in images:
            img_path = os.path.join(subdir_path, img)
            img_array = cv2.imread(img_path)
            image_arrays.append(img_array)

        image_arrays_np = np.array(image_arrays)
        video = torch.from_numpy(image_arrays_np).permute(0, 3, 1, 2).float()[:, [2, 1, 0], :, :]
        T, C, H, W = video.shape
        if idx + self.max_frames -1 <= T:
          video = video[idx:idx + self.max_frames]
        else:
          video = video[idx:]
          video = self.make_palindrome(video, self.max_frames)
        # video = video.permute(1, 0, 2, 3)
        # video = F.interpolate(video, size=self.interp_shape, mode="bilinear", align_corners=True)
        # video = video.reshape(T, 3, self.interp_shape[0], self.interp_shape[1])
        # return video,W,H
        # print(frames[-1])
        # if frames[-1] > self.max_video_frames:
        #   print("Less number of frames loaded")
        # if T < self.max_video_frames:
        #   last_frame = video[-1:]  # Get the last frame (shape: 1, C, H, W)
        #   num_replicates = self.max_video_frames - T  # Number of frames needed to reach 120
        #   replicated_frames = last_frame.repeat(num_replicates, 1, 1, 1)  # Replicate the last frame
        #   video = torch.cat([video, replicated_frames], dim=0)  # Concatenate replicated frames
        # else:
        #   video = video[:self.max_video_frames]
        # z = video.shape[0]
        video = F.interpolate(video, size=self.interp_shape, mode="bilinear", align_corners=True)
        video = video.reshape(self.max_frames, 3, self.interp_shape[0], self.interp_shape[1])  # Ensure the final shape is correct

        return video, W, H

    def load_anno(self, json_path):
        def create_input_tensor(annotation):
            keypoints = annotation['keypoints']
            processed_keypoints = []
            valids = []
            frame_no = annotation['image_id'] % 1000
            for i in range(0, len(keypoints), 3):
                x = keypoints[i]
                y = keypoints[i + 1]
                processed_keypoints.append([frame_no, x, y])
                visibility = keypoints[i + 2]
                valids.append(visibility)
            return torch.tensor(processed_keypoints), torch.tensor(valids).unsqueeze(0)

        def create_keypoints_tensor(annotation):
            keypoints = annotation['keypoints']
            processed_keypoints = []
            visibility = []
            frame_no = annotation['image_id'] % 1000
            for i in range(0, len(keypoints), 3):
                x = keypoints[i]
                y = keypoints[i + 1]
                vis = keypoints[i + 2]
                processed_keypoints.append([x, y])
                visibility.append(vis)
            return torch.tensor(processed_keypoints).unsqueeze(0), torch.tensor(visibility).unsqueeze(0)

        def best_starting_frame_at_least_18(frames):
          for i in range(len(frames)):
              start_frame = frames[i]
              end_frame = start_frame + self.max_frames -1
              count = sum(1 for frame in frames if start_frame <= frame <= end_frame)

              if count >= self.max_frames/2:
                  return start_frame, count

          return None, 0

        with open(json_path, 'r') as file:
            data = json.load(file)
            persons = {}
            frames = {}
            visibility = {}
            for i in data['annotations']:
                frame_num = i['image_id'] % 1000
                if i['person_id'] in persons:
                    new_annot, vis = create_keypoints_tensor(i)
                    persons[i['person_id']] = torch.cat((persons[i['person_id']], new_annot), dim=0)
                    frames[i['person_id']].append(frame_num)
                    visibility[i['person_id']] = torch.cat((visibility[i['person_id']], vis), dim=0)
                    if len(frames[i['person_id']]) == self.max_frames and frames[i['person_id']][0] + self.max_frames -1  == frames[i['person_id']][-1]:
                      initial = frames[i['person_id']][0]
                      input_frame = persons[initial]
                      frame_tensor = torch.full((17, 1), initial)
                      inputs = torch.cat((frame_tensor, input_frame), dim=1)
                      return inputs, persons[i['person_id']], visibility[i['person_id']], initial
                else:
                    # inputs[i['person_id']], valids[i['person_id']] = create_input_tensor(i)
                    persons[i['person_id']], visibility[i['person_id']] = create_keypoints_tensor(i)
                    frames[i['person_id']] = [frame_num]


            # If no person exceeds max_frames, return default values
            queries = None
            trajs_e = None
            start = None
            visibility = None
            for i in frames:
              frame_lst = frames[i]
              start, count = best_starting_frame_at_least_18(frame_lst)
              if not start is None:
                person = i
                initial_frame = start
                last_frame = start+self.max_frames
                for j in data['annotations']:
                  if j['image_id'] % 1000 >= initial_frame and j['image_id'] % 1000 < last_frame:
                    if j['person_id'] == person:
                      if trajs_e is None:
                        trajs_e, visibility = create_keypoints_tensor(j)
                      else:
                        new_annot, vis = create_keypoints_tensor(i)
                        trajs_e = torch.cat((trajs_e, new_annot), dim=0)
                        visibility = torch.cat((visibility, vis), dim=0)
                    else:
                      new_annot = torch.zeros((17, 2))
                      vis = torch.zeros(17)
                      trajs_e = torch.cat((trajs_e, new_annot), dim=0)
                      visibility = torch.cat((visibility, vis), dim=0)
                  initial_frame+=1
                  if initial_frame == last_frame:
                    break
                  if trajs_e.shape[0] != self.max_frames:
                    trajs_e = self.make_palindrome(trajs_e, self.max_frames)
                    visibility = self.make_palindrome(visibility, self.max_frames)

                input_frame = trajs_e[0]
                frame_tensor = torch.full((17, 1), 0)
                queries = torch.cat((frame_tensor, input_frame), dim=1)
                return queries, trajs_e, visibility, start

            if start is None:
              default_inputs = torch.zeros((17, 3))
              default_persons = torch.zeros((self.max_frames, 17, 2))
              default_visibility = torch.zeros((self.max_frames, 17))
              return default_inputs, default_persons, default_visibility, None

    def __getitem__(self, idx):
        subdir = self.valid_subdirectories[idx]
        img_path = os.path.join(self.main_folder, subdir)
        anno_path = os.path.join(self.json_folder, f"{subdir}.json")
        queries, trajs_e, visibility, start = self.load_anno(anno_path)
        # frames = self.load_anno(anno_path)
        if start is None:
          video,W,H = self.load_video(img_path,0)
        else:
          video,W,H = self.load_video(img_path,start)
        # video,W,H = self.load_video(img_path, frames)
        queries = queries.clone()
        queries[:, 1:] *= queries.new_tensor(
            [
                (self.interp_shape[1] - 1) / (W - 1),
                (self.interp_shape[0] - 1) / (H - 1),
            ]
        )
        queries = queries.clone()
        # Adjust tracks
        trajs_e = trajs_e.clone()
        trajs_e *= trajs_e.new_tensor(
            [
                (self.interp_shape[1] - 1) / (W - 1),
                (self.interp_shape[0] - 1) / (H - 1),
            ]
        )

        return video, queries, trajs_e, visibility

train_folder = '/content/drive/MyDrive/PoseTrack2/d1/images/train'
train_json_folder = '/content/drive/MyDrive/PoseTrack2/d1/PoseTrack21/posetrack_data/train'
train_dataset = PoseTrackDataset(train_folder, train_json_folder, 36, (384,512))

video, queries, trajs_e, visibility = train_dataset[100]

video.shape

