{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Making DataLoader"
      ],
      "metadata": {
        "id": "G_LiXzsjQ70F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haiwf-cXQ7Ay"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class PoseTrackDataset(Dataset):\n",
        "  def __init__(self, main_folder, json_folder):\n",
        "      self.main_folder = main_folder\n",
        "      self.json_folder = json_folder\n",
        "      self.subdirectories = sorted(next(os.walk(main_folder))[1])\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.subdirectories)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    subdir = self.subdirectories[idx]\n",
        "    img_path = os.path.join(self.main_folder, subdir)\n",
        "    anno_path = os.path.join(self.json_folder, f\"{subdir}.json\")\n",
        "    return img_path, anno_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_folder = '/content/drive/MyDrive/PoseTrack2/d1/images/train'\n",
        "train_json_folder = '/content/drive/MyDrive/PoseTrack2/d1/PoseTrack21/posetrack_data/train'\n",
        "val_folder = '/content/drive/MyDrive/PoseTrack2/d1/images/val'\n",
        "val_json_folder = '/content/drive/MyDrive/PoseTrack2/d1/PoseTrack21/posetrack_data/val'\n",
        "train_dataset = PoseTrackDataset(train_folder, train_json_folder)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
        "val_dataset = PoseTrackDataset(val_folder, val_json_folder)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "2W8FbNKNSjTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Model"
      ],
      "metadata": {
        "id": "zpxSrgV8O8vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/co-tracker\n",
        "%cd co-tracker\n",
        "!pip install -e .\n",
        "!pip install opencv-python einops timm matplotlib moviepy flow_vis\n",
        "!mkdir checkpoints\n",
        "%cd checkpoints\n",
        "!wget https://huggingface.co/facebook/cotracker/resolve/main/cotracker2.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbyJ5_juNyOW",
        "outputId": "6cdc3f09-0a3a-474c-8e07-f3f0cac09174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'co-tracker' already exists and is not an empty directory.\n",
            "/content/co-tracker\n",
            "Obtaining file:///content/co-tracker\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: cotracker\n",
            "  Attempting uninstall: cotracker\n",
            "    Found existing installation: cotracker 2.0\n",
            "    Uninstalling cotracker-2.0:\n",
            "      Successfully uninstalled cotracker-2.0\n",
            "  Running setup.py develop for cotracker\n",
            "Successfully installed cotracker-2.0\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.82)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: flow_vis in /usr/local/lib/python3.10/dist-packages (0.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cpu)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.34.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "mkdir: cannot create directory ‘checkpoints’: File exists\n",
            "/content/co-tracker/checkpoints\n",
            "--2024-06-08 17:50:13--  https://huggingface.co/facebook/cotracker/resolve/main/cotracker2.pth\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.88, 18.172.134.4, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/99/80/9980d78abf629546bc23aee3918967b68e7f1fe3a1bbbb105ecb0e930cf49b5b/362f5274376d610dc987b6daf2c2fefe63e06e1835f4ec1a10d0a15c5a4eef4f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27cotracker2.pth%3B+filename%3D%22cotracker2.pth%22%3B&Expires=1718128213&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODEyODIxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85OS84MC85OTgwZDc4YWJmNjI5NTQ2YmMyM2FlZTM5MTg5NjdiNjhlN2YxZmUzYTFiYmJiMTA1ZWNiMGU5MzBjZjQ5YjViLzM2MmY1Mjc0Mzc2ZDYxMGRjOTg3YjZkYWYyYzJmZWZlNjNlMDZlMTgzNWY0ZWMxYTEwZDBhMTVjNWE0ZWVmNGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=skwa217uqDPWC9Be16EZC1ofXqFww2wv7cdUSyquZd2t63-bHe8wEPW2VMM1-zIg-pS8ylWrV1AEHsN2ciWp3Vk2WzvRZxNCeoSssaVnSgWFC8bwGGEDaIRZAyCS1Cj-NrFgE6flvos4r7idnTBjASmzoaduMEaINHOcdfW7goA34EEIItC3tRQTqsUwkPBZLGTGIKOtMSRXz%7EmNIxozgqdeX0qmEyQNAkxjhJh%7EAIIhTvXtMrc25yrWneRGAqJy9u9uWqbr4uMVKhGUB0daO6RilCpxlE4n-x-OuOdNjCNBoP2brV%7EO38CAT0axK3opfnLgJMJW0gj62VgjytQbFg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-06-08 17:50:13--  https://cdn-lfs.huggingface.co/repos/99/80/9980d78abf629546bc23aee3918967b68e7f1fe3a1bbbb105ecb0e930cf49b5b/362f5274376d610dc987b6daf2c2fefe63e06e1835f4ec1a10d0a15c5a4eef4f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27cotracker2.pth%3B+filename%3D%22cotracker2.pth%22%3B&Expires=1718128213&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODEyODIxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85OS84MC85OTgwZDc4YWJmNjI5NTQ2YmMyM2FlZTM5MTg5NjdiNjhlN2YxZmUzYTFiYmJiMTA1ZWNiMGU5MzBjZjQ5YjViLzM2MmY1Mjc0Mzc2ZDYxMGRjOTg3YjZkYWYyYzJmZWZlNjNlMDZlMTgzNWY0ZWMxYTEwZDBhMTVjNWE0ZWVmNGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=skwa217uqDPWC9Be16EZC1ofXqFww2wv7cdUSyquZd2t63-bHe8wEPW2VMM1-zIg-pS8ylWrV1AEHsN2ciWp3Vk2WzvRZxNCeoSssaVnSgWFC8bwGGEDaIRZAyCS1Cj-NrFgE6flvos4r7idnTBjASmzoaduMEaINHOcdfW7goA34EEIItC3tRQTqsUwkPBZLGTGIKOtMSRXz%7EmNIxozgqdeX0qmEyQNAkxjhJh%7EAIIhTvXtMrc25yrWneRGAqJy9u9uWqbr4uMVKhGUB0daO6RilCpxlE4n-x-OuOdNjCNBoP2brV%7EO38CAT0axK3opfnLgJMJW0gj62VgjytQbFg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.154.185.94, 18.154.185.64, 18.154.185.27, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.154.185.94|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 204396415 (195M) [binary/octet-stream]\n",
            "Saving to: ‘cotracker2.pth.1’\n",
            "\n",
            "cotracker2.pth.1    100%[===================>] 194.93M   279MB/s    in 0.7s    \n",
            "\n",
            "2024-06-08 17:50:14 (279 MB/s) - ‘cotracker2.pth.1’ saved [204396415/204396415]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/co-tracker\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from base64 import b64encode\n",
        "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERRnAKbQPANY",
        "outputId": "783fb6e1-df37-4130-a0da-4c2f99f3b389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/co-tracker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cotracker.predictor1 import CoTrackerPredictor\n",
        "\n",
        "model = CoTrackerPredictor(\n",
        "    checkpoint=os.path.join(\n",
        "        './checkpoints/cotracker2.pth'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "4wO66v-bPDq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Annotations and Video"
      ],
      "metadata": {
        "id": "WKFfnTb7QNwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(subdir_path):\n",
        "  images = sorted([img for img in os.listdir(subdir_path) if img.endswith(\".jpg\")])\n",
        "\n",
        "  image_arrays = []\n",
        "  for img in images:\n",
        "      img_path = os.path.join(subdir_path, img)\n",
        "      img_array = cv2.imread(img_path)\n",
        "      image_arrays.append(img_array)\n",
        "\n",
        "  image_arrays_np = np.array(image_arrays)\n",
        "  video = torch.from_numpy(image_arrays_np).permute(0, 3, 1, 2)[None].float()[:, :, [2, 1, 0], :, :]\n",
        "\n",
        "  return video"
      ],
      "metadata": {
        "id": "-ZDDUqd4Qej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_anno(json_path):\n",
        "\n",
        "  def create_input_tensor(annotation):\n",
        "    keypoints = annotation['keypoints']\n",
        "    processed_keypoints = []\n",
        "    non_mask = []\n",
        "    frame_no = annotation['image_id']%1000\n",
        "    for i in range(0, len(keypoints), 3):\n",
        "      if(keypoints[i+2] == 1):\n",
        "        x = keypoints[i]\n",
        "        # y = 1920 - keypoints[i + 1] # Change 1920 for other datapoints\n",
        "        y = keypoints[i + 1]\n",
        "        non_mask.append(i)\n",
        "        processed_keypoints.append([frame_no, x, y])\n",
        "\n",
        "\n",
        "    return torch.tensor(processed_keypoints), non_mask\n",
        "\n",
        "  def create_keypoints_tensor(annotation, non_mask):\n",
        "    keypoints = annotation['keypoints']\n",
        "    processed_keypoints = []\n",
        "    frame_no = annotation['image_id']%1000\n",
        "    for i in non_mask:\n",
        "        x = keypoints[i]\n",
        "        y = keypoints[i + 1]\n",
        "        vis = keypoints[i+2]\n",
        "        processed_keypoints.append([x, y, vis])\n",
        "\n",
        "    return torch.tensor(processed_keypoints)\n",
        "\n",
        "  with open(json_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "  persons = {}\n",
        "  frames = {}\n",
        "  inputs = {}\n",
        "  mask = {}\n",
        "  output = {}\n",
        "  for i in data['annotations']:\n",
        "    # new_annot = create_keypoints_tensor1(i, False)\n",
        "    frame_num = i['image_id']%1000\n",
        "    if i['person_id'] in persons:\n",
        "      new_annot = create_keypoints_tensor(i,mask[i['person_id']])\n",
        "      persons[i['person_id']] = torch.cat((persons[i['person_id']], new_annot))\n",
        "      frames[i['person_id']].append(frame_num)\n",
        "      # valids[i['person_id']] = torch.cat((persons[i['person_id']], new_annot))\n",
        "    else:\n",
        "      inputs[i['person_id']], mask[i['person_id']] = create_input_tensor(i)\n",
        "      persons[i['person_id']] = create_keypoints_tensor(i,mask[i['person_id']])\n",
        "      frames[i['person_id']] = [frame_num]\n",
        "\n",
        "  return (inputs, persons, frames)\n"
      ],
      "metadata": {
        "id": "Gpyd1998RDq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "MUcECZGySZTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(actual, pred):\n",
        "  # Compute the errors\n",
        "  errors = torch.norm(actual[:, :2] - pred, dim=-1)\n",
        "\n",
        "  # Get the visibility flags\n",
        "  vis = actual[:, 2].reshape((-1))\n",
        "\n",
        "  # Filter the errors for visible points\n",
        "  visible_errors = errors[vis > 0]\n",
        "\n",
        "  # Compute the mean error for visible points\n",
        "  error = torch.mean(visible_errors)\n",
        "\n",
        "  return error"
      ],
      "metadata": {
        "id": "0_gkF1LXSn8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "EPS = 1e-6\n",
        "\n",
        "def reduce_masked_mean(x, mask, dim=None, keepdim=False):\n",
        "    \"\"\"\n",
        "    Compute the mean of x considering only the valid points defined by mask.\n",
        "\n",
        "    Args:\n",
        "    x (torch.Tensor): Data tensor.\n",
        "    mask (torch.Tensor): Mask tensor, should be the same shape as x.\n",
        "    dim (int or tuple of int, optional): Dimension(s) to reduce.\n",
        "    keepdim (bool, optional): Whether to keep the dimensions of reduction.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The mean value considering only valid points.\n",
        "    \"\"\"\n",
        "    # for (a, b) in zip(x.size(), mask.size()):\n",
        "    #     assert a == b  # Ensure shapes match\n",
        "    prod = x * mask\n",
        "    if dim is None:\n",
        "        numer = torch.sum(prod)\n",
        "        denom = EPS + torch.sum(mask)\n",
        "    else:\n",
        "        numer = torch.sum(prod, dim=dim, keepdim=keepdim)\n",
        "        denom = EPS + torch.sum(mask, dim=dim, keepdim=keepdim)\n",
        "\n",
        "    mean = numer / denom\n",
        "    return mean\n",
        "\n",
        "def evaluate_trajectories(trajs_e, trajs_g, valids, W, H):\n",
        "    \"\"\"\n",
        "    Evaluate the predicted trajectories against ground truth trajectories.\n",
        "\n",
        "    Args:\n",
        "    trajs_e (torch.Tensor): Predicted trajectories of shape (S, 2).\n",
        "    trajs_g (torch.Tensor): Ground truth trajectories of shape (S, 2).\n",
        "    valids (torch.Tensor): Validity mask of shape (S) with 1 for valid and 0 for invalid.\n",
        "    W (float): Width of the evaluation space.\n",
        "    H (float): Height of the evaluation space.\n",
        "\n",
        "    Returns:\n",
        "    dict: Metrics containing distance thresholds and average distance.\n",
        "    \"\"\"\n",
        "    # Distance thresholds\n",
        "    thrs = [1, 2, 4, 8, 16]\n",
        "    d_sum = 0.0\n",
        "    metrics = {}\n",
        "\n",
        "    # Scaling factors\n",
        "    sx_ = W / 256.0\n",
        "    sy_ = H / 256.0\n",
        "    sc_py = np.array([sx_, sy_]).reshape([1, 2])\n",
        "    sc_pt = torch.from_numpy(sc_py).float()\n",
        "\n",
        "    for thr in thrs:\n",
        "        # Calculate the L2 norm (Euclidean distance) and apply threshold\n",
        "        d_ = (torch.norm(trajs_e[1:] / sc_pt - trajs_g[1:] / sc_pt, dim=-1) < thr).float()  # Shape: (S-1)\n",
        "\n",
        "        # Reduce masked mean considering only valid points\n",
        "        d_ = reduce_masked_mean(d_, valids[1:]).item() * 100.0\n",
        "\n",
        "        # Accumulate the distance metrics\n",
        "        d_sum += d_\n",
        "\n",
        "        # Store individual threshold metrics\n",
        "        metrics['d_%d' % thr] = d_\n",
        "\n",
        "    # Calculate average distance metric\n",
        "    d_avg = d_sum / len(thrs)\n",
        "    metrics['d_avg'] = d_avg\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "alTDKbu0Dt5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.model.parameters(), lr=5e-4)\n"
      ],
      "metadata": {
        "id": "IJZ8tfMoTg55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_val_loss(model, val_dataloader, val_batches):\n",
        "  model.model.eval()\n",
        "  losses = []\n",
        "  metrics = {}\n",
        "  d1 = []\n",
        "  d2 = []\n",
        "  d4 = []\n",
        "  d8 = []\n",
        "  d16 = []\n",
        "  d_avg = []\n",
        "  with torch.no_grad():\n",
        "    for i in range(val_batches):\n",
        "      for batch_idx, k in enumerate(train_dataloader):\n",
        "        videos = []\n",
        "        annos = []\n",
        "        for video, anno in k:\n",
        "          if os.path.exists(video) and os.path.exists(anno):\n",
        "            videos.append(load_video(video))\n",
        "            annos.append(load_anno(anno))\n",
        "        total_loss = 0\n",
        "        for(video,y) in zip(videos, annos):\n",
        "          input = y[0]\n",
        "          output = y[1]\n",
        "          frames = y[2]\n",
        "          for i in input:\n",
        "            d1_temp = []\n",
        "            d2_temp = []\n",
        "            d4_temp = []\n",
        "            d8_temp = []\n",
        "            d16_temp = []\n",
        "            davg_temp = []\n",
        "            W = video.shape[-1]\n",
        "            H = video.shape[-2]\n",
        "            if torch.cuda.is_available():\n",
        "              model = model.cuda()\n",
        "              video = video.cuda()\n",
        "              input[i] = input[i].cuda()\n",
        "              output[i] = output[i].cuda()\n",
        "            pred_tracks, visibility = model(video, queries = input[i][None])\n",
        "            reshaped_output = pred_tracks[0][frames[i]] # Selecting the frames from output for calculating error\n",
        "            reshaped_output = reshaped_output.reshape((-1,2))\n",
        "            res_dict = evaluate_trajectories(reshaped_output, output[i][:,:2], output[i][:,2], W,H)\n",
        "            d1_temp.append(res_dict['d_1'])\n",
        "            d2_temp.append(res_dict['d_2'])\n",
        "            d4_temp.append(res_dict['d_4'])\n",
        "            d8_temp.append(res_dict['d_8'])\n",
        "            d16_temp.append(res_dict['d_16'])\n",
        "            davg_temp.append(res_dict['d_avg'])\n",
        "            total_loss += loss_fn(output[i], reshaped_output)\n",
        "          losses.append(total_loss.item()/len(input))\n",
        "          d1.append(sum(d1_temp)/len(d1))\n",
        "          d2.append(sum(d2_temp)/len(d2))\n",
        "          d4.append(sum(d4_temp)/len(d4))\n",
        "          d8.append(sum(d8_temp)/len(d8))\n",
        "          d16.append(sum(d16_temp)/len(d16))\n",
        "          d_avg.append(sum(davg_temp)/len(d_avg))\n",
        "\n",
        "    final_loss = sum(losses)/len(losses)\n",
        "    metrics['d_1'] = sum(d1)/len(d1)\n",
        "    metrics['d_2'] = sum(d2)/len(d2)\n",
        "    metrics['d_4'] = sum(d4)/len(d4)\n",
        "    metrics['d_8'] = sum(d8)/len(d8)\n",
        "    metrics['d_16'] = sum(d16)/len(d16)\n",
        "    metrics['d_avg'] = sum(d_avg)/len(d_avg)\n",
        "\n",
        "    print(f\"Validation loss: {final_loss:.4f}\")\n",
        "    for i in metrics:\n",
        "      print(f\"{i}: {metrics[i]:.4f}\")\n",
        "\n",
        "    model.model.train()\n",
        "    return"
      ],
      "metadata": {
        "id": "n4JEiF5jDrUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,optimizer,train_dataloader, val_freq, save_freq, num_epochs=10, ckpt_dir, use_augs = True):\n",
        "  step = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    for batch_idx, k in enumerate(train_dataloader):\n",
        "      videos = []\n",
        "      annos = []\n",
        "      for video, anno in k:\n",
        "        if os.path.exists(video) and os.path.exists(anno):\n",
        "          videos.append(load_video(video))\n",
        "          annos.append(load_anno(anno))\n",
        "      total_loss = 0\n",
        "      for(video,y) in zip(videos, annos):\n",
        "        input = y[0]\n",
        "        output = y[1]\n",
        "        frames = y[2]\n",
        "        if use_augs and np.random.rand() < 0.5: # rot90 aug\n",
        "          video = video.permute(0,1,2,4,3) # swap xy\n",
        "          output = output.flip([3])\n",
        "        for i in input:\n",
        "          if torch.cuda.is_available():\n",
        "            model = model.cuda()\n",
        "            video = video.cuda()\n",
        "            input[i] = input[i].cuda()\n",
        "            output[i] = output[i].cuda()\n",
        "            temp_loss = 0\n",
        "          # print(video.shape)\n",
        "          pred_tracks, visibility = model(video, queries = input[i][None])\n",
        "          reshaped_output = pred_tracks[0][frames[i]] # Selecting the frames from output for calculating error\n",
        "          reshaped_output = reshaped_output.reshape((-1,2))\n",
        "          temp_loss += loss_fn(output[i], reshaped_output)\n",
        "          print(\"Processed person\")\n",
        "        temp_loss = temp_loss/len(input)\n",
        "        total_loss += temp_loss\n",
        "      total_loss = total_loss/len(videos)\n",
        "      if torch.isnan(total_loss):\n",
        "        print('nan in loss; quitting')\n",
        "        return False\n",
        "      optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      print('Optimized')\n",
        "\n",
        "      if batch_idx % val_freq == 0:\n",
        "        compute_val_loss(model, val_dataloader)\n",
        "\n",
        "      if batch_idx % save_freq == 0:\n",
        "        torch.save(model.model.state_dict(), f'ckpt_dir/model_{batch_idx}_{step}.pth')\n",
        "        step+=1\n",
        "\n",
        "      break\n",
        "    break\n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Az7DhL8MSYkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}