# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tFVqbEtsrJNMXVCcqonnVXELE5RLSr9v
"""
from tensorboardX import SummaryWriter
from cotracker.predictor import CoTrackerPredictor
import torch
import os
import os
import json
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

import os
import requests

# Create the directory
checkpoint_dir = 'checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

# Change to the created directory
os.chdir(checkpoint_dir)

# URL of the file to be downloaded
url = 'https://huggingface.co/facebook/cotracker/resolve/main/cotracker2.pth'
filename = 'cotracker2.pth'

# Download the file
response = requests.get(url, stream=True)
if response.status_code == 200:
    with open(filename, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    print(f"Downloaded {filename} successfully.")
else:
    print(f"Failed to download {filename}. Status code: {response.status_code}")

# Optionally, change back to the original directory if needed
# os.chdir('..')


def loss_fn(actual, pred):
    # Compute the errors
    errors = torch.norm(actual[:, :2] - pred, dim=-1)

    # Get the visibility flags
    vis = actual[:, 2].reshape((-1))

    # Filter the errors for visible points
    visible_errors = errors[vis > 0]

    # Compute the mean error for visible points
    error = torch.mean(visible_errors)

    return error

model = CoTrackerPredictor(
    checkpoint=os.path.join(
        './checkpoints/cotracker2.pth'
    )
)

EPS = 1e-6

class PoseTrackDataset(Dataset):
    def __init__(self, main_folder, json_folder):
        self.main_folder = main_folder
        self.json_folder = json_folder
        self.subdirectories = sorted(next(os.walk(main_folder))[1])

    def __len__(self):
        return len(self.subdirectories)

    def create_input_tensor(self, annotation):
      keypoints = annotation['keypoints']
      processed_keypoints = []
      non_mask = []
      frame_no = annotation['image_id']%1000
      for i in range(0, len(keypoints), 3):
        if(keypoints[i+2] == 1):
          x = keypoints[i]
          # y = 1920 - keypoints[i + 1] # Change 1920 for other datapoints
          y = keypoints[i + 1]
          non_mask.append(i)
          processed_keypoints.append([frame_no, x, y])


      return torch.tensor(processed_keypoints), non_mask

    def create_keypoints_tensor(self, annotation, non_mask):
      keypoints = annotation['keypoints']
      processed_keypoints = []
      frame_no = annotation['image_id']%1000
      for i in non_mask:
          x = keypoints[i]
          y = keypoints[i + 1]
          vis = keypoints[i+2]
          processed_keypoints.append([x, y, vis])

      return torch.tensor(processed_keypoints)

    def __getitem__(self, idx):
        subdir = self.subdirectories[idx]
        subdir_path = os.path.join(self.main_folder, subdir)
        images = sorted([img for img in os.listdir(subdir_path) if img.endswith(".jpg")])

        image_arrays = []
        for img in images:
            img_path = os.path.join(subdir_path, img)
            img_array = cv2.imread(img_path)
            image_arrays.append(img_array)

        image_arrays_np = np.array(image_arrays)
        video = torch.from_numpy(image_arrays_np).permute(0, 3, 1, 2).float()[:, [2, 1, 0], :, :]

        json_path = os.path.join(self.json_folder, f"{subdir}.json")
        with open(json_path, 'r') as file:
            data = json.load(file)
        persons = {}
        frames = {}
        inputs = {}
        mask = {}
        output = {}
        for i in data['annotations']:
          # new_annot = create_keypoints_tensor1(i, False)
          frame_num = i['image_id']%1000
          if i['person_id'] in persons:
            new_annot = self.create_keypoints_tensor(i,mask[i['person_id']])
            persons[i['person_id']] = torch.cat((persons[i['person_id']], new_annot))
            frames[i['person_id']].append(frame_num)
            # valids[i['person_id']] = torch.cat((persons[i['person_id']], new_annot))
          else:
            inputs[i['person_id']], mask[i['person_id']] = self.create_input_tensor(i)
            persons[i['person_id']] = self.create_keypoints_tensor(i,mask[i['person_id']])
            frames[i['person_id']] = [frame_num]

        output['persons'] = persons
        output['frames'] = frames
        output['inputs'] = inputs
        output['video'] = video

        return output

train_folder = '/content/drive/MyDrive/PoseTrack2/d1/images/train'
train_json_folder = '/content/drive/MyDrive/PoseTrack2/d1/PoseTrack21/posetrack_data/train'
val_folder = '/content/drive/MyDrive/PoseTrack2/d1/images/val'
val_json_folder = '/content/drive/MyDrive/PoseTrack2/d1/PoseTrack21/posetrack_data/val'
train_dataset = PoseTrackDataset(train_folder, train_json_folder)
train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=0)
val_dataset = PoseTrackDataset(val_folder, val_json_folder)
val_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=True, num_workers=0)

def reduce_masked_mean(x, mask, dim=None, keepdim=False):
    """
    Compute the mean of x considering only the valid points defined by mask.

    Args:
    x (torch.Tensor): Data tensor.
    mask (torch.Tensor): Mask tensor, should be the same shape as x.
    dim (int or tuple of int, optional): Dimension(s) to reduce.
    keepdim (bool, optional): Whether to keep the dimensions of reduction.

    Returns:
    torch.Tensor: The mean value considering only valid points.
    """
    # for (a, b) in zip(x.size(), mask.size()):
    #     assert a == b  # Ensure shapes match
    prod = x * mask
    if dim is None:
        numer = torch.sum(prod)
        denom = EPS + torch.sum(mask)
    else:
        numer = torch.sum(prod, dim=dim, keepdim=keepdim)
        denom = EPS + torch.sum(mask, dim=dim, keepdim=keepdim)

    mean = numer / denom
    return mean

def evaluate_trajectories(trajs_e, trajs_g, valids, W, H):
    """
    Evaluate the predicted trajectories against ground truth trajectories.

    Args:
    trajs_e (torch.Tensor): Predicted trajectories of shape (S, 2).
    trajs_g (torch.Tensor): Ground truth trajectories of shape (S, 2).
    valids (torch.Tensor): Validity mask of shape (S) with 1 for valid and 0 for invalid.
    W (float): Width of the evaluation space.
    H (float): Height of the evaluation space.

    Returns:
    dict: Metrics containing distance thresholds and average distance.
    """
    # Distance thresholds
    thrs = [1, 2, 4, 8, 16]
    d_sum = 0.0
    metrics = {}

    # Scaling factors
    sx_ = W / 256.0
    sy_ = H / 256.0
    sc_py = np.array([sx_, sy_]).reshape([1, 2])
    sc_pt = torch.from_numpy(sc_py).float()

    for thr in thrs:
        # Calculate the L2 norm (Euclidean distance) and apply threshold
        d_ = (torch.norm(trajs_e[1:] / sc_pt - trajs_g[1:] / sc_pt, dim=-1) < thr).float()  # Shape: (S-1)

        # Reduce masked mean considering only valid points
        d_ = reduce_masked_mean(d_, valids[1:]).item() * 100.0

        # Accumulate the distance metrics
        d_sum += d_

        # Store individual threshold metrics
        metrics['d_%d' % thr] = d_

    # Calculate average distance metric
    d_avg = d_sum / len(thrs)
    metrics['d_avg'] = d_avg

    return metrics

import torch

def compute_val_loss(model, val_dataloader, device):
    model.eval()  # Set the model to evaluation mode
    val_loss = 0.0
    d_1, d_2, d_4, d_8, d_16, d_avg = [], [], [], [], [], []

    with torch.no_grad():  # No need to track gradients for validation
        for batch_idx, d in enumerate(val_dataloader):
            video = d['video'].to(device)
            inputs = d['inputs'].to(device)
            persons = d['persons'].to(device)
            frames = d['frames'].to(device)
            mean_errors = []
            W = video.shape[-1]
            H = video.shape[-2]
            for i in inputs:
                queries = inputs[i]
                if torch.cuda.is_available():
                    model = model.cuda()
                    video = video.cuda()
                    queries = queries.cuda()
                pred_tracks, pred_visibility = model(video, queries=queries[None])
                k = pred_tracks[0][frames[i]]  # Selecting the frames from output for calculating error
                k = k.reshape((-1, 2))
                mse = loss_fn(persons[i], k)
                mean_errors.append(mse)
                res_dict = evaluate_trajectories(k, persons[i][:, :2], persons[i][:, 2], W, H)
                d_1.append(res_dict['d_1'])
                d_2.append(res_dict['d_2'])
                d_4.append(res_dict['d_4'])
                d_8.append(res_dict['d_8'])
                d_16.append(res_dict['d_16'])
                d_avg.append(res_dict['d_avg'])

            average = sum(mean_errors) / len(mean_errors)
            val_loss += average.item()

    val_loss /= len(val_dataloader)
    metrics = {
        'd_1': sum(d_1) / len(d_1),
        'd_2': sum(d_2) / len(d_2),
        'd_4': sum(d_4) / len(d_4),
        'd_8': sum(d_8) / len(d_8),
        'd_16': sum(d_16) / len(d_16),
        'd_avg': sum(d_avg) / len(d_avg)
    }

    return val_loss, metrics

def train(model, train_dataloader, optimizer,device, epochs):

  writer = SummaryWriter()
  for epoch in range(epochs):
    for batch_idx, d in train_dataloader:
      video = d['video'].to(device)
      inputs = d['inputs'].to(device)
      persons = d['persons'].to(device)
      frames = d['frames'].to(device)
      mean_errors = []
      d_1 = []
      d_2 = []
      d_4 = []
      d_8 = []
      d_16 = []
      d_avg = []
      W = video.shape[-1]
      H = video.shape[-2]
      metrics = {}
      for i in inputs:
        queries = inputs[i]
        if torch.cuda.is_available():
          model = model.cuda()
          video = video.cuda()
          queries = queries.cuda()
        pred_tracks, pred_visibility = model(video, queries=queries[None])
        k = pred_tracks[0][frames[i]] # Selecting the frames from output for calculating error
        k = k.reshape((-1,2))
        mse = loss_fn(persons[i],k)
        mean_errors.append(mse)
        res_dict = evaluate_trajectories(k, persons[i][:,:2], persons[i][:,2], W,H)
        d_1.append(res_dict['d_1'])
        d_2.append(res_dict['d_2'])
        d_4.append(res_dict['d_4'])
        d_8.append(res_dict['d_8'])
        d_16.append(res_dict['d_16'])
        d_avg.append(res_dict['d_avg'])

      average = sum(mean_errors) / len(mean_errors)
      metrics['d_1'] = sum(d_1)/len(d_1)
      metrics['d_2'] = sum(d_2)/len(d_2)
      metrics['d_4'] = sum(d_4)/len(d_4)
      metrics['d_8'] = sum(d_8)/len(d_8)
      metrics['d_16'] = sum(d_16)/len(d_16)
      metrics['d_avg'] = sum(d_avg)/len(d_avg)

      avg_loss = torch.stack(mean_errors)
      loss = torch.mean(avg_loss, dim=0)

      optimizer.zero_grad()
      loss.backard()
      optimizer.step()
      writer.add_scalar('Training Loss', loss.item(), epoch * len(train_dataloader) + batch_idx)
      writer.add_scalar('Training Accuracy', metrics['d_avg'], epoch * len(train_dataloader) + batch_idx)
      print(f"Loss: {loss.item()}")
      for i in metrics:
        print(f"{i}: {metrics[i]}")

    if i % 100 == 0:
      val_loss, val_metrics = compute_val_loss(model, val_dataloader,device)  # Define this function
      writer.add_scalar('Validation Loss', val_loss, epoch * len(val_dataloader) + batch_idx)
      writer.add_scalar('Validation Accuracy', val_metrics['d_avg'], epoch * len(val_dataloader) + batch_idx)
      print(f"Validation Loss: {val_loss}")
      for i in val_metrics:
        print(f"{i}: {val_metrics[i]}")